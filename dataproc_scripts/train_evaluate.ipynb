{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-2-m.us-east1-c.c.valued-odyssey-330617.internal:46135\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3423ef0b80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "import os, tempfile\n",
    "tmp = tempfile.NamedTemporaryFile(delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket parameters. Remember to update it with the same values as the entryfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BUCKET=\"neoway-challenge4\"\n",
    "BIGQUERY_BUCKET=\"bigquery-tempbucket-nc4\"\n",
    "SCRIPTS_BUCKET=\"dataproc_shin_scripts4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define run id for saving artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = datetime.strftime(datetime.now(), \"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup bigquery bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('temporaryGcsBucket', BIGQUERY_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing default spark conf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(f\"gs://{DATA_BUCKET}/raw/churned_customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from feature stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_features = spark.read.format('bigquery').option('table', 'feature_store.customers_churned_before').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_features = spark.read.format('bigquery').option('table', 'feature_store.calendar_features').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Date_index\", f.date_format(f.col(\"Onboard_Date\"), \"yyyyMMdd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.\\\n",
    "join(customer_features, [\"Names\"], \"inner\")\\\n",
    ".join(calendar_features, ['Date_index'], \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_dict = [{\"col_name\": \"Age\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Total_Purchase\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Account_Manager\", \"type\": \"string\"},\n",
    "             {\"col_name\": \"Years\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Num_Sites\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Location\", \"type\": \"string\"},\n",
    "             {\"col_name\": \"Company\", \"type\": \"string\"},\n",
    "             {\"col_name\": \"Onboard_Year\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_Month\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_DayOfMonth\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_DayOfWeek\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_Quarter\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_WeekOfYear\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Churned_Before\", \"type\": \"string\"},\n",
    "             {\"col_name\": \"Churn\", \"type\": \"double\"},\n",
    "            ]\n",
    "\n",
    "for c in cast_dict:\n",
    "    df = df.withColumn(c[\"col_name\"], f.col(c[\"col_name\"]).cast(c[\"type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Churn\"\n",
    "categorical_variables = list(set([i['col_name'] for i in cast_dict if i['type'] == 'string']) - set([target]))\n",
    "numerical_variables = list(set([i['col_name'] for i in cast_dict if i['type'] == 'double']) - set([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weight column\n",
    "It will be used to deal with imbalanced labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"weight\", f.when(f.col(target) == 1, f.lit(5)).otherwise(f.lit(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select useful columns in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.select(*[i['col_name'] for i in cast_dict] + [target, \"weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_indexed\", handleInvalid=\"keep\") for col in categorical_variables]\n",
    "onehotencoders = [OneHotEncoder(inputCols=[col+\"_indexed\"], outputCols=[col+\"_encoded\"], dropLast=False, handleInvalid=\"keep\") for col in categorical_variables]\n",
    "vectorassemblers = [VectorAssembler(inputCols=[col], outputCol=col+\"_vectorized\") for col in numerical_variables]\n",
    "standardscalers = [StandardScaler(inputCol=col+\"_vectorized\", outputCol=col+\"_standarized\", withMean=False, withStd=True) for col in numerical_variables]\n",
    "finalassembler = VectorAssembler(inputCols=list(set([var + \"_encoded\" for var in categorical_variables]) | set([var + \"_standarized\" for var in numerical_variables])), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = indexers\n",
    "stages.extend(onehotencoders + vectorassemblers + standardscalers)\n",
    "stages.extend([finalassembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting training data to preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=stages).fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data according to the pipeline and select only useful columns\n",
    "Note that all the variables are now represented as a \"features\" vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pipeline.transform(train).select(\"features\", target, \"weight\")\n",
    "test = pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model with hyperparameter search and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=target, featuresCol=\"features\", weightCol=\"weight\", seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=target, metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder() \\\n",
    "  .addGrid(rf.maxDepth, [6, 16]) \\\n",
    "  .addGrid(rf.maxBins, [5]) \\\n",
    "  .addGrid(rf.numTrees, [12]) \\\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=grid, numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Save the params from the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = {\"maxBins\": cvModel.bestModel._java_obj.getMaxBins(),\n",
    "              \"maxDepth\": cvModel.bestModel._java_obj.getMaxDepth(), \n",
    "              \"numTrees\": cvModel.bestModel._java_obj.getNumTrees()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(DATA_BUCKET)\n",
    "blob = bucket.blob(f'run/{run_id}/rf_best_params.json')\n",
    "blob.upload_from_string(data=json.dumps(rf_best_params), content_type='application/json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save prediction object for post analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "prediction.write.format('parquet').mode(\"overwrite\").save(f\"gs://{DATA_BUCKET}/run/{run_id}/test_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and save metrics from the best model using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "prediction_to_evaluate = prediction.withColumn(\"label\", f.col(target).cast(\"double\")).select(\"prediction\", \"label\")\n",
    "metrics = MulticlassMetrics(prediction_to_evaluate.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "tp = confusion_matrix[0][0]\n",
    "fn = confusion_matrix[1][0]\n",
    "fp = confusion_matrix[0][1]\n",
    "tn = confusion_matrix[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "rf_metrics = {\"AreaUnderROC\": BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderROC\", weightCol=\"weight\").evaluate(prediction),\n",
    "\"AreaUnderPR\": BinaryClassificationEvaluator(labelCol=target, metricName=\"areaUnderPR\", weightCol=\"weight\").evaluate(prediction),\n",
    "\"Precision\": tp/(tp+fp),\n",
    "\"Recall\": tp/(tp+fn),\n",
    "\"FMeasure\": (2*tp)/((2*tp)+fp+fn),\n",
    "\"FalsePositiveRate\":fp/(fn+tn),\n",
    "\"ConfusionMatrix\": confusion_matrix.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'run/{run_id}/rf_metrics.json')\n",
    "blob.upload_from_string(data=json.dumps(rf_metrics), content_type='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
