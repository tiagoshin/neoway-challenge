{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "import os, tempfile\n",
    "tmp = tempfile.NamedTemporaryFile(delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket parameters. Remember to update it with the same values as the entryfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BUCKET=\"neoway-challenge3\"\n",
    "BIGQUERY_BUCKET=\"bigquery-tempbucket-nc3\"\n",
    "SCRIPTS_BUCKET=\"dataproc_shin_scripts3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from onnxmltools import convert_sparkml\n",
    "from onnxmltools.convert.sparkml.utils import buildInitialTypesSimple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup bigquery bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('temporaryGcsBucket', BIGQUERY_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing default spark conf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df = spark.read.parquet(f\"gs://{DATA_BUCKET}/raw/churned_customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from feature stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_features = spark.read.format('bigquery').option('table', 'feature_store.customers_churned_before').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_features = spark.read.format('bigquery').option('table', 'feature_store.calendar_features').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Date_index\", f.date_format(f.col(\"Onboard_Date\"), \"yyyyMMdd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.\\\n",
    "join(customer_features, [\"Names\"], \"inner\")\\\n",
    ".join(calendar_features, ['Date_index'], \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_dict = [{\"col_name\": \"Age\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Total_Purchase\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Account_Manager\", \"type\": \"string\"},\n",
    "             {\"col_name\": \"Years\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Num_Sites\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Location\", \"type\": \"string\"},\n",
    "             {\"col_name\": \"Company\", \"type\": \"string\"},\n",
    "             {\"col_name\": \"Onboard_Year\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_Month\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_DayOfMonth\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_DayOfWeek\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_Quarter\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Onboard_WeekOfYear\", \"type\": \"double\"},\n",
    "             {\"col_name\": \"Churned_Before\", \"type\": \"string\"},\n",
    "             {\"col_name\": \"Churn\", \"type\": \"double\"},\n",
    "            ]\n",
    "\n",
    "for c in cast_dict:\n",
    "    df = df.withColumn(c[\"col_name\"], f.col(c[\"col_name\"]).cast(c[\"type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(*[i['col_name'] for i in cast_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Churn\"\n",
    "categorical_variables = list(set([i['col_name'] for i in cast_dict if i['type'] == 'string']) - set([target]))\n",
    "numerical_variables = list(set([i['col_name'] for i in cast_dict if i['type'] == 'double']) - set([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weight column\n",
    "It will be used to deal with imbalanced labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"weight\", f.when(f.col(target) == 1, f.lit(5)).otherwise(f.lit(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_indexed\", handleInvalid=\"keep\") for col in categorical_variables]\n",
    "onehotencoders = [OneHotEncoder(inputCols=[col+\"_indexed\"], outputCols=[col+\"_encoded\"], dropLast=False, handleInvalid=\"keep\") for col in categorical_variables]\n",
    "vectorassemblers = [VectorAssembler(inputCols=[col], outputCol=col+\"_vectorized\") for col in numerical_variables]\n",
    "standardscalers = [StandardScaler(inputCol=col+\"_vectorized\", outputCol=col+\"_standarized\", withMean=False, withStd=True) for col in numerical_variables]\n",
    "finalassembler = VectorAssembler(inputCols=list(set([var + \"_encoded\" for var in categorical_variables]) | set([var + \"_standarized\" for var in numerical_variables])), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get params from the best rf model and create the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(DATA_BUCKET)\n",
    "list_of_runs = sorted([int(b.name[4:18]) for b in bucket.list_blobs() if b.name[0:3] == \"run\"], reverse=True)\n",
    "last_run = list_of_runs[0]\n",
    "blob = bucket.blob(f'run/{last_run}/rf_best_params.json')\n",
    "rf_best_params = json.loads(blob.download_as_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'maxBins': 5, 'maxDepth': 6, 'numTrees': 12}\n"
     ]
    }
   ],
   "source": [
    "print(rf_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=target, featuresCol=\"features\", weightCol=\"weight\", \n",
    "                            maxBins=rf_best_params[\"maxBins\"], \n",
    "                            maxDepth=rf_best_params[\"maxDepth\"],\n",
    "                            numTrees=rf_best_params[\"numTrees\"],\n",
    "                            seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = indexers\n",
    "stages.extend(onehotencoders + vectorassemblers + standardscalers)\n",
    "stages.extend([finalassembler, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting training data to preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=stages).fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving pipeline for inference using onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_types = buildInitialTypesSimple(df.select(categorical_variables+numerical_variables).limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 8.                               \n"
     ]
    }
   ],
   "source": [
    "onnx_model = convert_sparkml(pipeline, 'Pyspark pipeline model', initial_types, spark_session = spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob('model/onnx_pipeline.onnx')\n",
    "blob.upload_from_string(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
